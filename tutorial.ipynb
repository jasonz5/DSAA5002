{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity and Dissimilarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 常见Metrics\n",
    "- Similarity:\n",
    "  - Jaccard / cosine similarity: For sparse data (e.g., documents) \n",
    "  - Pearson’s correlation: For linear relationships\n",
    "  - Mutual information: For none-linear relationship\n",
    "- Dissimilarity:\n",
    "  - Hamming distance: For binary vector\n",
    "  - Euclidean distance: For dense data (e.g., time series, multi-dimensional points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 各种相似度计算\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "from scipy.stats import pearsonr # type: ignore\n",
    "from sklearn.metrics import mutual_info_score # type: ignore\n",
    "\n",
    "def sim_dissim_numpy(a,b):\n",
    "    array1 = np.asarray(a, dtype=bool)\n",
    "    array2 = np.asarray(b, dtype=bool)\n",
    "    intersection = np.sum(np.logical_and(array1, array2))\n",
    "    union = np.sum(np.logical_or(array1, array2))\n",
    "    jaccard_similarity =  intersection / union if union != 0 else 0\n",
    "    cosine_similarity = np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "    pearson_correlation = np.corrcoef(a, b)[0, 1]\n",
    "    mutual_information = mutual_info_score(a, b)\n",
    "    hamming_distance = np.sum(a != b)\n",
    "    euclidean_distance = np.linalg.norm(a - b)\n",
    "    manhattan_distance = np.sum(np.abs(a - b))\n",
    "    supremum_distance = np.max(np.abs(a - b))\n",
    "    print(\"Jaccard Similarity:\", jaccard_similarity)\n",
    "    print(\"Cosine Similarity:\", cosine_similarity)\n",
    "    print(\"Pearson's Correlation:\", pearson_correlation)\n",
    "    print(\"Mutual Information:\", mutual_information)\n",
    "    print(\"Hamming Distance:\", hamming_distance)\n",
    "    print(\"Euclidean Distance:\", euclidean_distance)\n",
    "    print(\"Manhattan Distance:\", manhattan_distance)\n",
    "    print(\"Supremum (Chebyshev) Distance:\", supremum_distance)\n",
    "def sim_dissim_scipy(a,b):\n",
    "    array1 = np.asarray(a, dtype=bool)\n",
    "    array2 = np.asarray(b, dtype=bool)\n",
    "    jaccard_similarity =  1 - distance.jaccard(array1, array2)\n",
    "    cosine_similarity = 1 - distance.cosine(a, b)\n",
    "    pearson_correlation, _ = pearsonr(a, b)\n",
    "    mutual_information = mutual_info_score(a, b)\n",
    "    hamming_distance = distance.hamming(a, b) * len(a)\n",
    "    euclidean_distance = distance.euclidean(a, b)\n",
    "    manhattan_distance = distance.cityblock(a, b)\n",
    "    supremum_distance = distance.chebyshev(a, b)\n",
    "    print(\"Jaccard Similarity:\", jaccard_similarity)\n",
    "    print(\"Cosine Similarity:\", cosine_similarity)\n",
    "    print(\"Pearson's Correlation:\", pearson_correlation)\n",
    "    print(\"Mutual Information:\", mutual_information)\n",
    "    print(\"Hamming Distance:\", hamming_distance)\n",
    "    print(\"Euclidean Distance:\", euclidean_distance)\n",
    "    print(\"Manhattan Distance:\", manhattan_distance)\n",
    "    print(\"Supremum (Chebyshev) Distance:\", supremum_distance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test\n",
    "a = [1, 2, 3, 4, 5]\n",
    "b = [5, 4, 3, 2, 1]\n",
    "a = np.array(a)\n",
    "b = np.array(b)\n",
    "sim_dissim_numpy(a,b)\n",
    "sim_dissim_scipy(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 相似度计算分数转换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cosine Similarity => Euclidean Distance\n",
    "\n",
    "    假设有两个向量 $ \\mathbf{A} $ 和 $ \\mathbf{B} $，它们的余弦相似度为 $ \\text{cos\\_sim}(\\mathbf{A}, \\mathbf{B}) $。余弦相似度和欧几里得距离之间的关系如下：\n",
    "    $$\n",
    "    \\text{Euclidean Distance} = \\sqrt{2 \\cdot (1 - \\text{cos\\_sim}(\\mathbf{A}, \\mathbf{B}))}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def cosine_similarity_to_euclidean_distance(cosine_similarity, vector_length1=1, vector_length2=1):\n",
    "    if vector_length1==1 and vector_length2==1:\n",
    "        euclidean_distance = np.sqrt(2 * (1 - cosine_similarity))\n",
    "    else:\n",
    "        euclidean_distance = np.sqrt(vector_length1**2 + vector_length2**2 - 2 * vector_length1 * vector_length2 * cosine_similarity)\n",
    "    return euclidean_distance\n",
    "\n",
    "def euclidean_distance_to_cosine_similarity(euclidean_distance, vector_length1=1, vector_length2=1):\n",
    "    if vector_length1==1 and vector_length2==1:\n",
    "        cosine_similarity = 1 - (euclidean_distance**2 / 2)\n",
    "    else:\n",
    "        cosine_similarity = (vector_length1**2 + vector_length2**2 - euclidean_distance**2) / (2 * vector_length1 * vector_length2)\n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim = 0.8\n",
    "vector_length1 = 1\n",
    "vector_length2 = 1\n",
    "\n",
    "# 计算欧几里得距离\n",
    "euclidean_dist = cosine_similarity_to_euclidean_distance(cos_sim, vector_length1, vector_length2)\n",
    "print(\"欧几里得距离:\", euclidean_dist)\n",
    "\n",
    "# 将欧几里得距离转换回余弦相似度\n",
    "cosine_sim_converted = euclidean_distance_to_cosine_similarity(euclidean_dist, vector_length1, vector_length2)\n",
    "print(\"转换后的余弦相似度:\", cosine_sim_converted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基本介绍\n",
    "- 常见归一化方式\n",
    "  - Z-score normalization: Mean=0, Standard deviation=1\n",
    "  - Min-max normalization: Rescaled into any given interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def normalizaition(array, minmaxRange = (0,1)):\n",
    "    # Z-score normalization\n",
    "    zscore_normalized_array = zscore(array)\n",
    "\n",
    "    # Min-max normalization\n",
    "    scaler = MinMaxScaler(feature_range=minmaxRange) # 默认缩放至[0,1]\n",
    "    minmax_normalized_array = scaler.fit_transform(array.reshape(-1, 1)).flatten()\n",
    "\n",
    "    print(\"Original Array:\", array)\n",
    "    print(\"Z-score Normalized Array:\", zscore_normalized_array)\n",
    "    print(\"Min-Max Normalized Array:\", minmax_normalized_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = [100,200,400,800,1500,3000]\n",
    "array= np.array(array)\n",
    "minmaxRange = (0,1)\n",
    "normalizaition(array, minmaxRange=minmaxRange)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基本知识\n",
    "+ Goals:\n",
    "  + To determine the best attribute for splitting the data\n",
    "  + To stop splitting when further division doesn't improve the model\n",
    "+ Methods\n",
    "  + Splitting criterion:\n",
    "    + Gini Index\n",
    "    + Entropy\n",
    "    + Classification Error\n",
    "  + Stop splitting criterion:\n",
    "    + Only one class in a node \n",
    "    + No available attributes \n",
    "    + No samples in a node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gini Index\n",
    "#### 基本定义\n",
    "- 用途：Measure Impurity of a Single node\n",
    "- 数学定义：$ Gini = 1 - \\sum_{i=1}^{n} p_i^2 $\n",
    "  - $n$ is the total number of classes,\n",
    "  - $p_i$ is the proportion of samples belonging to class $i$ in the node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 案例一：Identify the best attribute condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "## 第一步：Calculate the Gini index of a root node.\n",
    "Label = np.array([5,5])\n",
    "G_ori = 1 - np.sum((Label/np.sum(Label))**2)\n",
    "print(\"Gini index of root node: \", G_ori)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 第二步：确定Which attribute would be chosen as the first splitting attribute?\n",
    "def get_gini_spilt(M):\n",
    "    row_sums = M.sum(axis=1, keepdims=True)\n",
    "    normalized_M = M / row_sums\n",
    "    squared_M = normalized_M ** 2\n",
    "    gini_each = 1 - squared_M.sum(axis=1)\n",
    "    ratio_each = M.sum(axis=1)/np.sum(M.sum(axis=1))\n",
    "    gini_split = np.sum(gini_each * ratio_each)\n",
    "    return gini_split\n",
    "\n",
    "A = np.array([[4,2],[1,3]])\n",
    "G_A = get_gini_spilt(A)\n",
    "B = np.array([[3,3],[2,2]])\n",
    "G_B = get_gini_spilt(B)\n",
    "print(\"gini_spilt of A: \", G_A)\n",
    "print(\"gini_spilt of B: \", G_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 第三步：Calculate the information gain.\n",
    "GAIN_A = G_ori - G_A\n",
    "GAIN_B = G_ori - G_B\n",
    "print(\"GAIN_A:\", GAIN_A)\n",
    "print(\"GAIN_B:\", GAIN_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 案例二：推导决策树\n",
    "- 根据混淆矩阵评估决策树效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "confusion_matrix = np.array([[50, 10],\n",
    "                             [5, 35]])\n",
    "\n",
    "# 从混淆矩阵中获取TP, TN, FP, FN\n",
    "TP = confusion_matrix[0, 0]  # 真阳性\n",
    "TN = confusion_matrix[1, 1]  # 真阴性\n",
    "FP = confusion_matrix[1, 0]  # 假阳性\n",
    "FN = confusion_matrix[0, 1]  # 假阴性\n",
    "\n",
    "accuracy = (TP + TN) / np.sum(confusion_matrix)\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "f_measure = 2 * (precision * recall) / (precision + recall)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F-measure: {f_measure:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means\n",
    "### 基本知识\n",
    "- 算法迭代过程\n",
    "  1. **初始化**：随机选择 k 个观察值作为初始的聚类中心。\n",
    "  2. **分配步骤**：将每个观察值分配给最近的聚类中心，形成 k 个簇。\n",
    "  3. **更新步骤**：对每个簇，计算所有点的均值并将其设置为新的聚类中心。\n",
    "  4. **重复**：重复分配和更新步骤，直到聚类中心不再显著变化或达到预定的迭代次数，算法结束。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 案例一：\n",
    "#### 迭代过程可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# K-means聚类的逐步实现函数\n",
    "def kmeans_1d_until_convergence(points, initial_centroids):\n",
    "    centroids = initial_centroids.copy()\n",
    "    steps_data = []\n",
    "    step = 0\n",
    "    converged = False\n",
    "\n",
    "    while not converged:\n",
    "        step += 1\n",
    "        \n",
    "        # 第一步：将点分配到最近的质心\n",
    "        distances = np.abs(points[:, np.newaxis] - centroids)\n",
    "        cluster_assignments = np.argmin(distances, axis=1)\n",
    "        \n",
    "        # 第二步：根据分配的点的平均值更新质心\n",
    "        new_centroids = []\n",
    "        for k in range(len(centroids)):\n",
    "            assigned_points = points[cluster_assignments == k]\n",
    "            if len(assigned_points) > 0:\n",
    "                new_centroids.append(np.mean(assigned_points))\n",
    "            else:\n",
    "                new_centroids.append(centroids[k])  # 如果没有分配的点，质心保持不变\n",
    "        new_centroids = np.array(new_centroids)\n",
    "        \n",
    "        # 记录当前步的数据\n",
    "        step_data = {\n",
    "            \"Step\": step,\n",
    "            \"Centroids\": new_centroids,\n",
    "            \"Cluster Assignments\": [points[cluster_assignments == i].tolist() for i in range(len(centroids))]\n",
    "        }\n",
    "        steps_data.append(step_data)\n",
    "        \n",
    "        # 检查是否收敛（如果质心不再变化）\n",
    "        if np.array_equal(centroids, new_centroids):\n",
    "            converged = True\n",
    "        else:\n",
    "            centroids = new_centroids  # 更新质心\n",
    "    \n",
    "    # 将步骤数据转换为DataFrame便于查看\n",
    "    steps_df = pd.DataFrame({\n",
    "        \"Step\": [d[\"Step\"] for d in steps_data],\n",
    "        \"Cluster Assignments\": [d[\"Cluster Assignments\"] for d in steps_data],\n",
    "        \"Centroids\": [d[\"Centroids\"] for d in steps_data],\n",
    "    })\n",
    "    \n",
    "    return steps_df\n",
    "\n",
    "# 样本数据点和初始质心\n",
    "points = np.array([100, 200, 400, 800, 1100, 1600])  # 输入的一维数据点\n",
    "initial_centroids = np.array([1100, 1600])  # 初始质心\n",
    "\n",
    "# 运行函数并展示结果\n",
    "steps_df = kmeans_1d_until_convergence(points, initial_centroids)\n",
    "print(steps_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 计算SSE和SSB值\n",
    "SSE Formula:\n",
    "The Sum of Squared Error within cluster (SSE) measures the total variance within each cluster:\n",
    "$$\n",
    "\\text{SSE} = \\sum_{i=1}^{k} \\sum_{x \\in C_i} \\| x - \\mu_i \\|^2\n",
    "$$\n",
    "\n",
    "SSB Formula:\n",
    "The Sum of Squares Between clusters (SSB) measures the variance between the clusters and the overall mean:\n",
    "$$\n",
    "\\text{SSB} = \\sum_{i=1}^{k} |C_i| \\cdot \\| \\mu_i - \\mu \\|^2\n",
    "$$\n",
    "\n",
    "Explanation:\n",
    "- $ k $: The number of clusters.\n",
    "- $ C_i $: The set of points in cluster $ i $.\n",
    "- $ x $: Individual data points within a cluster.\n",
    "- $ \\mu_i $: The centroid (mean) of cluster $ i $.\n",
    "- $ \\mu $: The overall mean of all data points.\n",
    "- $ |C_i| $: The number of points in cluster $ i $.\n",
    "\n",
    "These formulas calculate the within-cluster error (SSE) and between-cluster variation (SSB) in clustering analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sse_ssb(points, centroids, cluster_assignments):\n",
    "    # 计算整体均值（所有点的均值）\n",
    "    overall_mean = np.mean(points)\n",
    "    \n",
    "    # 计算 SSE\n",
    "    sse = 0\n",
    "    for k in range(len(centroids)):\n",
    "        assigned_points = points[cluster_assignments == k]\n",
    "        sse += np.sum((assigned_points - centroids[k]) ** 2)\n",
    "    \n",
    "    # 计算 SSB\n",
    "    ssb = 0\n",
    "    for k in range(len(centroids)):\n",
    "        assigned_points = points[cluster_assignments == k]\n",
    "        cluster_size = len(assigned_points)\n",
    "        ssb += cluster_size * (centroids[k] - overall_mean) ** 2\n",
    "    \n",
    "    return sse, ssb\n",
    "\n",
    "# 获取最终的质心和分配结果\n",
    "final_centroids = steps_df[\"Centroids\"].iloc[-1]\n",
    "cluster_assignments = np.argmin(np.abs(points[:, np.newaxis] - final_centroids), axis=1)\n",
    "\n",
    "# 计算 SSE 和 SSB\n",
    "sse, ssb = calculate_sse_ssb(points, final_centroids, cluster_assignments)\n",
    "print(\"Sum of Squared Error (SSE):\", sse)\n",
    "print(\"Sum of Squares Between clusters (SSB):\", ssb)\n",
    "# SSE = (100-375)**2 +(200-375)**2 +(400-375)**2 + (800-375)**2 +(1100-1350)**2 +(1600-1350)**2\n",
    "# SSB = 4 * (700-375)**2 + 2 * (700-1350)**2\n",
    "# print(SSE) # SSE: 412500\n",
    "# print(SSB) # SSB: 1267500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 计算 Silhouette Coefficient\n",
    "Silhouette Coefficient 是衡量一个点的聚类质量的指标，其取值范围为 -1 到 1。Silhouette Coefficient 的定义公式如下：\n",
    "$$\n",
    "s = \\frac{b - a}{\\max(a, b)}\n",
    "$$\n",
    "\n",
    "其中：\n",
    "- $ a $：该点到其所属簇中其他点的平均距离，即点到同簇内部的紧密度。\n",
    "- $ b $：该点到最近的另一个簇的所有点的平均距离，即点到最近其他簇的分离度。\n",
    "\n",
    "Silhouette Coefficient 的解释如下：\n",
    "- **正值**：表明该点更接近其所属的簇，而远离其他簇，意味着聚类效果较好。\n",
    "- **零值**：表示该点位于两个簇的边界上，聚类效果不显著。\n",
    "- **负值**：表明该点可能更适合属于其他簇，聚类效果较差。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def silhouette_coefficient(point, points, cluster_assignments, centroids):\n",
    "    # 找到该点的簇\n",
    "    point_cluster = cluster_assignments[np.where(points == point)[0][0]]\n",
    "    \n",
    "    # 计算 a：该点到同簇其他点的平均距离\n",
    "    same_cluster_points = points[cluster_assignments == point_cluster]\n",
    "    if len(same_cluster_points) > 1:\n",
    "        # 排除该点自身\n",
    "        a = np.mean(np.abs(same_cluster_points[same_cluster_points != point] - point))\n",
    "    else:\n",
    "        a = 0  # 若簇中仅有该点，则 a 为 0\n",
    "    \n",
    "    # 计算 b：该点到最近的其他簇的平均距离\n",
    "    b = np.min([np.mean(np.abs(points[cluster_assignments == k] - point)) for k in range(len(centroids)) if k != point_cluster])\n",
    "    \n",
    "    # 计算 Silhouette Coefficient\n",
    "    silhouette = (b - a) / max(a, b)\n",
    "    return silhouette\n",
    "\n",
    "\n",
    "point = 100\n",
    "silhouette_point = silhouette_coefficient(point, points, cluster_assignments, final_centroids)\n",
    "print(\"Silhouette Coefficient for point:\", silhouette_point)\n",
    "# a = ((200-100)+ (400-100)+(800-100))/3\n",
    "# b = ((1600 - 100) + (1100 - 100))/2\n",
    "# s = 1-a/b\n",
    "# print(s) # 0.7066666666666667"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 案例二：\n",
    "A distance matrix is given in the following Table. Points p1 and p2 belong to cluster 1, and points p3 and p4 belong to cluster 2.\n",
    "\n",
    "#### 计算SSE\n",
    "$ \\text{SSE} = \\sum_{i=1}^{k} \\sum_{x \\in C_i} \\| x - \\mu_i \\|^2 $\n",
    "\n",
    "$ X1-m=X1-(X1+X2)/2=(X1-X2)/2=d(p1,p2)/2 $\n",
    "\n",
    "$ \\text{SSE} = (d(p1,p2)/2)^2 + (d(p1,p2)/2)^2 + (d(p3,p4)/2)^2 + (d(p3,p4)/2)^2   $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 计算 Silhouette Coefficient\n",
    "$ s = \\frac{b - a}{\\max(a, b)} $\n",
    "- $ a $：簇内平均距离，\n",
    "- $ b $：最近簇的所有点的平均距离"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 案例三：\n",
    "Compute the entropy and purity for each cluster and the overall clustering result, according to the confusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "\n",
    "def calculate_entropy_purity(confusion_matrix):\n",
    "    # 转换成NumPy数组\n",
    "    confusion_matrix = np.array(confusion_matrix)\n",
    "    \n",
    "    # 总的样本数\n",
    "    total_samples = np.sum(confusion_matrix)\n",
    "    \n",
    "    # 初始化熵和纯度\n",
    "    cluster_entropies = []\n",
    "    total_entropy = []\n",
    "    cluster_purities = []\n",
    "    total_purity = []\n",
    "    \n",
    "    # 计算每个Cluster的熵和纯度\n",
    "    for i, cluster in enumerate(confusion_matrix):\n",
    "        # 每个cluster的样本数\n",
    "        cluster_total = np.sum(cluster)\n",
    "        \n",
    "        # 计算熵\n",
    "        cluster_entropy = entropy(cluster, base=2) if cluster_total > 0 else 0\n",
    "        cluster_entropies.append(cluster_entropy)\n",
    "        total_entropy.append((cluster_total / total_samples) * cluster_entropy)\n",
    "        \n",
    "        # 计算纯度\n",
    "        cluster_purity = np.max(cluster) / cluster_total if cluster_total > 0 else 0\n",
    "        cluster_purities.append(cluster_purity)\n",
    "        total_purity.append((cluster_total / total_samples) * cluster_purity)\n",
    "    \n",
    "    # 总熵和总纯度\n",
    "    total_entropy = sum(total_entropy)\n",
    "    total_purity = sum(total_purity)\n",
    "    \n",
    "    return {\n",
    "        \"cluster_entropies\": cluster_entropies,\n",
    "        \"total_entropy\": total_entropy,\n",
    "        \"cluster_purities\": cluster_purities,\n",
    "        \"total_purity\": total_purity\n",
    "    }\n",
    "\n",
    "# 示例混淆矩阵\n",
    "confusion_matrix = [\n",
    "    [1,1,0,11,4,676],\n",
    "    [27,89,333,827,253,33],\n",
    "    [326,465,8,105,16,29]\n",
    "]\n",
    "\n",
    "# 计算熵和纯度\n",
    "results = calculate_entropy_purity(confusion_matrix)\n",
    "print(\"每个Cluster的熵:\", [f\"{entropy:.4f}\" for entropy in results[\"cluster_entropies\"]])\n",
    "print(\"总熵:\", f\"{results['total_entropy']:.4f}\")\n",
    "print(\"每个Cluster的纯度:\", [f\"{purity:.4f}\" for purity in results[\"cluster_purities\"]])\n",
    "print(\"总纯度:\", f\"{results['total_purity']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierachical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agglomerative Clustering（凝聚层次聚类）\n",
    "Agglomerative Clustering 是一种层次聚类算法，从每个数据点作为一个独立的簇开始，通过不断地合并最近的簇，逐步形成层次化的聚类结构，直到满足停止条件。以下是算法的步骤：\n",
    "\n",
    "1. **初始化**：将每个数据点视为一个单独的簇，开始时有 $ n $ 个簇，$ n $ 是数据点的数量。\n",
    "\n",
    "2. **计算距离矩阵**：计算每对簇之间的距离，通常使用欧氏距离，但可以选择其他距离度量（如曼哈顿距离、余弦相似度等）。\n",
    "\n",
    "3. **合并最近的簇**：找到距离最小的两簇，将它们合并为一个新的簇。然后更新距离矩阵，将新簇与其他簇之间的距离重新计算。\n",
    "   \n",
    "   - **距离更新方法**：\n",
    "      - **单连接（Single Linkage）**：新簇与其他簇之间的距离是两个簇间最近点的距离。\n",
    "      - **全连接（Complete Linkage）**：新簇与其他簇之间的距离是两个簇间最远点的距离。\n",
    "      - **平均连接（Average Linkage）**：新簇与其他簇之间的距离是两个簇间所有点对距离的平均值。\n",
    "      - **质心连接（Centroid Linkage）**：新簇的质心与其他簇质心之间的距离。\n",
    "   \n",
    "4. **更新簇数量**：将簇的数量减1（因为合并了两个簇），然后返回步骤3继续合并。\n",
    "\n",
    "5. **重复合并**：不断重复步骤3和步骤4，直到达到预定的停止条件。常见的停止条件包括：\n",
    "   - 达到预定义的簇数量。\n",
    "   - 距离阈值超出设定的范围。\n",
    "   - 层次树结构形成完成。\n",
    "\n",
    "6. **结果输出**：最终形成的聚类结果可以表示为一个**层次树（dendrogram）**，从树的不同层次可以看到数据在不同尺度上的聚类结构。根据需求可以选择不同的层次来划分簇。\n",
    "\n",
    "> **注意**：Agglomerative Clustering 适用于小规模数据集，计算量随数据规模增加而显著增加，因此不适合大规模数据集。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 案例题："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.cluster.hierarchy as sch\n",
    "import pandas as pd\n",
    "\n",
    "# Given points\n",
    "points = np.array([100, 200, 400, 800, 1100, 1600]).reshape(-1, 1)\n",
    "\n",
    "# Perform hierarchical clustering with single linkage (MIN)\n",
    "linkage_matrix = sch.linkage(points, method='single')\n",
    "\n",
    "# Display the merge operations and distances for each step in the linkage matrix\n",
    "linkage_matrix_df = {\n",
    "    'Cluster 1': linkage_matrix[:, 0].astype(int),\n",
    "    'Cluster 2': linkage_matrix[:, 1].astype(int),\n",
    "    'Distance': linkage_matrix[:, 2],\n",
    "    'New Cluster Size': linkage_matrix[:, 3].astype(int)\n",
    "}\n",
    "linkage_matrix_df = pd.DataFrame(linkage_matrix_df)\n",
    "print(\"Hierarchical Clustering Merge Operations:\")\n",
    "print(linkage_matrix_df)\n",
    "\n",
    "# Plot the dendrogram\n",
    "plt.figure(figsize=(8, 5))\n",
    "dendrogram = sch.dendrogram(linkage_matrix, labels=points.flatten(), orientation='top')\n",
    "plt.title('Hierarchical Clustering Dendrogram (Single Linkage)')\n",
    "plt.xlabel('Points')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node Importance\n",
    "- Degree Centrality\n",
    "- Closeness Centrality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
